# Отчет

## Изменения
1) В ```unet``` модели вектор эмбедингов temb расширен и правильно бродкастится.
2) В ```diffusion```  не стал менеять system design модели, где в forward считатеся лосс (ачепоч), добавил возвращаемые выходы ```x_t``` и подправил под эту функциональность оставльной проект. Исправл багу в формуле ```x_t```: ```one_minus_alpha_over_prod -> sqrt_one_minus_alpha_prod``` согласно формулам диффузии.
3) Добавил инструмент ```poetry``` и соотвествующие конфиг-файлы для удобства работы и починки конфликтов в зависимостях.
4) Отрефакторил функции в ```main.py``` для удобного парсинга ```hydra```-конфига.

## Запуск

Добавил поддержку ```hydra``` и закинул конфиг с дефолтными значениями в папку conf.

Можно запускать базово через ```python main.py```. 

Можно добавлять аргументы: ```python main.py trainer.batch_size=64 trainer.num_epochs=50 trainer.peak_lr=1e-3 trainer.flip_augment=true```


## Тесты

1) Зафксировал сид для проверки лосса в тесте ```test_diffusion``` это позволяет сохранять верным предположение о правильно имплементированной модели и обеспечивает воспроивзодимость.

2) Собрал пайплайн (интеграционный тест) для проверки модулей из training. Прогнал на случайном шуме, чтобы проверить работоспрособность. Параметризовал гиперпараметры модейлей. Параметризовал возможные девайсы, что позволило отсечь сценарии, когда тензоры в процессе работы лежат на разных девайсах.

## wandb

Проект с ранами и артефактами: https://wandb.ai/byz0vtm-ural-federal-university/effdl?nw=nwuserbyz0vtm

P.S. Модель не сошлась, пофиксить перебором гиперпараметров и поиском баг так и не смог. Так что в ранах есть большое разнообразие конфигов в артефактах, хотя и показательного рана на 100 эпох не имеется, модель прееобучалась сильно раньше.