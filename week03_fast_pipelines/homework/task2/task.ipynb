{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler, IterableDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer import PositionalEncoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 640\n",
    "DATA_PATH = \"./data/test-00000-of-00001.txt\"\n",
    "BATCH_SIZE = 32\n",
    "device = \"cuda:1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2LikeModel(nn.Module):\n",
    "    def __init__(self, vocab_size: int, d_model: int = 1024, nhead: int = 8, max_length: int = MAX_LENGTH):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_len=max_length)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=d_model, nhead=nhead)\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=1)\n",
    "        self.output_linear = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, attn_mask: torch.Tensor = None) -> torch.Tensor:\n",
    "        # x: [batch, seq_len]\n",
    "        x = self.embedding(x)             # [batch, seq_len, d_model]\n",
    "        x = self.positional_encoding(x)\n",
    "        x = x.transpose(0, 1)             # [seq_len, batch, d_model]\n",
    "        # In this mock setup we use x as both tgt and memory.\n",
    "        out = self.decoder(tgt=x, memory=x, tgt_mask=attn_mask)\n",
    "        out = out.transpose(0, 1)         # [batch, seq_len, d_model]\n",
    "        logits = self.output_linear(out)  # [batch, seq_len, vocab_size]\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(loader: DataLoader, model: nn.Module, device: torch.device) -> np.ndarray:\n",
    "    times = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "\n",
    "            input_ids = batch.to(device)\n",
    "            attn_mask = None\n",
    "\n",
    "            torch.cuda.synchronize()\n",
    "            start_event = torch.cuda.Event(enable_timing=True)\n",
    "            end_event = torch.cuda.Event(enable_timing=True)\n",
    "            start_event.record()\n",
    "            _ = model(input_ids, attn_mask)\n",
    "            end_event.record()\n",
    "            torch.cuda.synchronize()\n",
    "            elapsed = start_event.elapsed_time(end_event)  # milliseconds\n",
    "            times.append(elapsed)\n",
    "    return np.array(times)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/byzovti/.conda/envs/effdl2/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "def get_tokenizer():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})\n",
    "    return tokenizer\n",
    "\n",
    "tokenizer = get_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_stats(times: np.ndarray):\n",
    "    return {\n",
    "        \"min (ms)\": float(np.min(times)),\n",
    "        \"max (ms)\": float(np.max(times)),\n",
    "        \"mean (ms)\": float(np.mean(times)),\n",
    "        \"median (ms)\": float(np.median(times)),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warmup(model: GPT2LikeModel, device: torch.device):\n",
    "    model.eval()\n",
    "    dummy_input = torch.randint(0, model.vocab_size, (BATCH_SIZE, MAX_LENGTH), device=device)\n",
    "    with torch.no_grad():\n",
    "        for _ in range(150):\n",
    "            _ = model(dummy_input)\n",
    "    torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LikeModel(\n",
       "  (embedding): Embedding(30522, 1024)\n",
       "  (positional_encoding): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): TransformerDecoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerDecoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (multihead_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=1024, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (output_linear): Linear(in_features=1024, out_features=30522, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPT2LikeModel(vocab_size=tokenizer.vocab_size, d_model=1024, nhead=8, max_length=MAX_LENGTH).to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "warmup(model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (523 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'min (ms)': 1.3834240436553955,\n",
       " 'max (ms)': 2.5661120414733887,\n",
       " 'mean (ms)': 1.6852821137878922,\n",
       " 'median (ms)': 1.6493760347366333}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dataset import BrainDataset\n",
    "\n",
    "brain_dataset = BrainDataset(DATA_PATH, tokenizer, max_length=MAX_LENGTH)\n",
    "loader_brain = DataLoader(brain_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "compute_stats(run_epoch(loader_brain, model, device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'min (ms)': 1.3087040185928345,\n",
       " 'max (ms)': 3.008352041244507,\n",
       " 'mean (ms)': 1.8467498926016002,\n",
       " 'median (ms)': 1.840127944946289}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dataset import BigBrainDataset, manual_collate_fn\n",
    "\n",
    "\n",
    "big_brain_dataset = BigBrainDataset(DATA_PATH, tokenizer)\n",
    "loader_big_brain = DataLoader(big_brain_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=lambda b: manual_collate_fn(b, max_length=None))\n",
    "\n",
    "compute_stats(run_epoch(loader_big_brain, model, device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k is 1\n",
      "{'min (ms)': 1.2513279914855957, 'max (ms)': 7.106272220611572, 'mean (ms)': 1.8758553355193335, 'median (ms)': 1.7817599773406982}\n",
      "-------------\n",
      "k is 5\n",
      "{'min (ms)': 1.1518080234527588, 'max (ms)': 2.3552958965301514, 'mean (ms)': 1.5966621752370869, 'median (ms)': 1.6250880360603333}\n",
      "-------------\n",
      "k is 10\n",
      "{'min (ms)': 1.1460479497909546, 'max (ms)': 3.546112060546875, 'mean (ms)': 1.6330742886086471, 'median (ms)': 1.6866240501403809}\n",
      "-------------\n",
      "k is 20\n",
      "{'min (ms)': 1.1130880117416382, 'max (ms)': 2.2750720977783203, 'mean (ms)': 1.5971747859049652, 'median (ms)': 1.5279040336608887}\n",
      "-------------\n",
      "k is 50\n",
      "{'min (ms)': 1.1447999477386475, 'max (ms)': 2.4433600902557373, 'mean (ms)': 1.6895174891495508, 'median (ms)': 1.698815941810608}\n",
      "-------------\n"
     ]
    }
   ],
   "source": [
    "from dataset import UltraBigBrainBatchSampler, UltraBigBrainDataset, manual_collate_fn\n",
    "\n",
    "\n",
    "ultra_big_brain_dataset = UltraBigBrainDataset(\n",
    "    DATA_PATH,\n",
    "    tokenizer\n",
    ")\n",
    "\n",
    "for k in (1, 5, 10, 20, 50):\n",
    "\n",
    "    batch_sampler = UltraBigBrainBatchSampler(ultra_big_brain_dataset, batch_size=BATCH_SIZE, k=20)\n",
    "\n",
    "    ultra_big_brain_loader = DataLoader(\n",
    "        ultra_big_brain_dataset,\n",
    "        batch_sampler=batch_sampler,\n",
    "        collate_fn=lambda b: manual_collate_fn(b, max_length=None)\n",
    "    )\n",
    "    print(f\"k is {k}\")\n",
    "    print(compute_stats(run_epoch(ultra_big_brain_loader, model, device)))\n",
    "    print(\"-------------\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "effdl2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
